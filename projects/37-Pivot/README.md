# 基本资料
 项目名称：PIVOT
 
 项目立项日期： 20230615
 

# 项目简介：Layer1的项目模型开源应用
AGI压缩了整个互联网的数据，而整个互联网由全人类共同构建也从始至终都就由全人类共同拥有，AGI理应继续由全人类共同拥有。

任何法律一经制定就已经滞后，根本来不及管理AGI这个突发的庞然大物，AGI只能交由全人类共同管理。

就像每个人类小孩都要上学，AGI模型开源从某种意义上就是让全人类来共建一所专供AGI上学的学校。
 
 
 ## AGI去中心化的重要性
 OpenAI就是一个屠龙少年终成ClosedAI的故事。只要世界还一天按照Web2的方式运行，屠龙少年的故事就会永远重复下去。拥抱Web3化是唯一一劳永逸的解法。
 
 反垄断 
 
 
  信息透明
  
  
AGI应该代表谁的立场


低延时响应、隐私计算、边缘计算

  重复造轮子与碳足迹
  
  ## Foundation Model
  Foundation Model指的是一种已经经过训练并能够处理某种任务（比如文本分类、图像识别等）的模型，通常它的参数是通过大规模数据集进行无监督或有监督的学习而得到的。例如，在自然语言处理领域，BERT (Bidirectional Encoder Representations from Transformers) 就是一个被广泛使用的基础模型，其通过对海量文本进行预训练，得到了一个强大的语言表示能力。

而Fine-tuning则是在基础模型的基础上，通过少量的标注数据对模型进行微调，以适应不同的任务。通常情况下，Fine-tuning都包含以下几个步骤：

1. 选择一个合适的基础模型；
2. 根据需要的任务添加新的层或修改已有的层；
3. 使用少量的标注数据对整个模型进行训练。

## Nostr-AGI协议
### AGI

- AGI模型数据由4部分数据组成（模型代码、模型参数、训练数据、验证数据），每个部分独立迭代升级。除了模型代码外另外三个都有巨大的数据量，所以要做分片（sharding）处理。
- 每部数据（模型参数、训练数据、验证数据）经过纠错码处理后拆分成k份，分布冗余储存在n个中继器里。从概率来说，用户只要从所有n个中继器里随机找出m个，就能完全还原完整的数据。(n>m>k)
    - 如果是用户的私人AGI模型，先对它进行加密后再经过纠错码处理后拆分成k份，分布冗余储存在n个中继器里。
- 数据主要分为两类：全人类共有的Foundation model和私人model。
    - Foundation model （巨大）分片储存在全网所有中继器里
    - 私人model（较小）分片或不分片储存在一部分中继器里
- 私人的domain specific model之所以较小因为它指定某一版/当前版（version）的Foundation model作为master，然后私人model自己作为其小分岔（fork），储存数据时只储存私人model与Foundation model的$\Delta = dsm-fm$，所以得到一个稀疏矩阵（sparse）。

### Nostr

- Nostr 有两个组件：客户端和中继器。每个用户运行一个客户端，任何人都可以运行中继器。
- 每个用户都由公钥标识，每数据都有签名，每个客户端都会验证这些签名。
- 在启动时，客户端从它知道的所有m个中继器中）。
- 客户端从他们选择并信任的m个中继器中查询获取（fetch）它所关注的数据（Foundation model and/or domain specific model
- 并将根据Foundation model数据fine-tune后的私人DSM的数据发布到他们选择并的其他中继器。
- 中继器不与另一个中继器通信，仅直接与用户通信。
 ### 激励机制

- 训练数据由DAO共献提供，无激励。
- 验证数据由DAO共献提供，无激励。
- 任何能推动Foundation model进化的人，都将获得PoWoC挖矿奖励token，这些token再作为PoS挖矿并成为主要的出块来源。推动Foundation model进化的人必需公开计算细节以供DAO审核重现，可被重现且符合规则的结果才会被共识。
    
    [PoWoC的量化方式](https://www.notion.so/PoWoC-9c7ff55a61f042e48eb5f985d7e8b427?pvs=21)
    
- DAO会空投很多SBT治理代币给德高望重的AGI学者。
 
 
